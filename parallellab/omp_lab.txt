I started with kernel_seq as a baseline and optimized that as much as I could, mainly moving expensive computations outside the inner for loop. I then added two outer for loops to allow me to iterate over blocks of the overall matrix. Once that was confirmed to work, I modified the outer 2 for loops to iterate diagonally as opposed to row-wise (this now only covered the upper-left half of the matrix). I then allowed the diagonally traversing loop to run in parallel. After confirming that this worked, I added a second (near-identical) set of for loops after the first to account for the lower-right half of the matrix. Now that I could iterate over the matrix blocks diagonally (and in parallel) and the elements row-wise, all that was left was to tune my BLOCK_SIZE. I found that 64 and 128 had good results, but 64 had the better of the two. Ideally I think somewhere between these 2 values would have worked best, but the code was simpler to implement using a power of 2 BLOCK_SIZE, so I stuck with 64.
A last minute change I added was to use the current value and current north value as the next iteration's west and north-west values. This further improved efficiency and got my run time down by a fair bit.
